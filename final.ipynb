{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4299869b",
   "metadata": {},
   "source": [
    "# Proyecto: Clasificación de Productos con Aprendizaje por Refuerzo (Q-Learning)\n",
    "\n",
    "Este proyecto simula una cinta transportadora como las que se encuentran en fábricas o plantas industriales.  \n",
    "Un agente inteligente aprende a tomar decisiones por sí mismo sobre:\n",
    "\n",
    "- Cuándo retirar un producto defectuoso.\n",
    "- Cuándo dejar pasar un producto en buen estado.\n",
    "\n",
    "Para esto se utiliza **Aprendizaje por Refuerzo**, específicamente el algoritmo **Q-Learning**, donde el agente aprende mediante recompensas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8c73b",
   "metadata": {},
   "source": [
    "# pregunta de final  \n",
    "\n",
    "\n",
    "Modifica el código para que se considere un tercer producto que se desplace a otro contenedor, el cual corresponde a un producto que se puede corregirlo posteriormente\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a378da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356703b7",
   "metadata": {},
   "source": [
    "## Entorno del Proyecto\n",
    "\n",
    "El entorno representa una cinta transportadora con productos:  \n",
    "(estados)\n",
    "- 0 → producto bueno\n",
    "- 1 → producto defectuoso\n",
    "\n",
    "Acciones del agente:\n",
    "- 0 = dejar pasar\n",
    "- 1 = retirar\n",
    "\n",
    "Recompensas:\n",
    "- +10 si toma la decisión correcta\n",
    "- -5 si se equivoca\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7318b97",
   "metadata": {},
   "source": [
    "ahora estoy agregando uno que es  \n",
    "0 -> bueno  \n",
    "1 -> reparable  \n",
    "2 -> malo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fcaf55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CintaTransportadoraEnv:\n",
    "\n",
    "#agregaremoos otro para el producto que se pude reparar , con una probabilidad del 20%s\n",
    "    def __init__(self, prob_malo=0.3,prob_reparable=0.2, semilla=None):\n",
    "        self.prob_malo = prob_malo\n",
    "        self.prob_reparable= prob_reparable\n",
    "        self.rng = random.Random(semilla)\n",
    "        self.producto_actual = None\n",
    "\n",
    "    def reiniciar(self):\n",
    "        \n",
    "        rando= self.rng.random()\n",
    "        \n",
    "        #vemos la probabilidad de que sea bueno\n",
    "        \n",
    "        prob_bueno= 1.0 - self.prob_reparable - self.prob_malo\n",
    "        \n",
    "        if rando < prob_bueno:\n",
    "            \n",
    "            self.producto_actual = 0 \n",
    "        elif rando < prob_bueno + self.prob_reparable:\n",
    "            \n",
    "            self.producto_actual = 1 \n",
    "        else:\n",
    "            self.producto_actual = 2  \n",
    "        \n",
    "        return self.producto_actual\n",
    "        \n",
    "        #self.producto_actual = 1 if self.rng.random() < self.prob_malo else 0\n",
    "        #return self.producto_actual\n",
    "\n",
    "    def paso(self, accion):\n",
    "        \n",
    "        \n",
    "        \n",
    "        #0  dejar pasar\n",
    "        #1  enviar a reparación\n",
    "        #2  desechar\n",
    "     \n",
    "   \n",
    "        prod = self.producto_actual\n",
    "        \n",
    "        if accion == 0:  \n",
    "            if prod == 0:   \n",
    "                recompensa = 5\n",
    "            elif prod == 1:  \n",
    "                recompensa = -8\n",
    "            else:            \n",
    "                recompensa = -15\n",
    "                \n",
    "        elif accion == 1:  \n",
    "            if prod == 0:    \n",
    "                recompensa = -3\n",
    "            elif prod == 1:  \n",
    "                recompensa = 8\n",
    "            else:            \n",
    "                recompensa = -5\n",
    "                \n",
    "        else:  \n",
    "            if prod == 0:   \n",
    "                recompensa = -5\n",
    "            elif prod == 1:  \n",
    "                recompensa = -2\n",
    "            else:            \n",
    "                recompensa = 10\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #if accion == 1:  # retirar\n",
    "        #    recompensa = 10.0 if prod == 1 else -5.0\n",
    "        #else:            # dejar pasar\n",
    "        #    recompensa = 5.0 if prod == 0 else -10.0\n",
    "\n",
    "        #self.producto_actual = 1 if self.rng.random() < self.prob_malo else 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #reiniciamos todo y generamos el producto siguiente\n",
    "        self.producto_actual = self.reiniciar()\n",
    "        terminado = False\n",
    "        info = {\"producto\": prod}\n",
    "        return self.producto_actual, recompensa, terminado, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1540c5",
   "metadata": {},
   "source": [
    "## Algoritmo Q-Learning\n",
    "\n",
    "El agente aprende una tabla Q con valores para cada acción.  \n",
    "Actualiza la tabla con la fórmula:\n",
    "\n",
    "Q(s,a) = Q(s,a) + α [ r + γ max(Q(s’, a’)) − Q(s,a) ]\n",
    "\n",
    "Con esto aprende qué hacer en cada estado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100118c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenamiento(episodios, alpha, gamma,epsilon, decay_epsilon, min_epsilon,prob_reparable ,prob_malo, semilla, guardar_ruta):\n",
    "    env = CintaTransportadoraEnv(prob_reparable=prob_reparable,prob_malo=prob_malo,semilla=semilla)\n",
    "   #ahora tendremos 3 acciones y 3 estados eso es lo qye cambie\n",
    "    n_estados=3\n",
    "    n_acciones=3\n",
    "    \n",
    "    #iniciamos en ceros\n",
    "    Q = np.zeros((n_estados,n_acciones),dtype=np.float64)\n",
    "    historial_recompensas=[]\n",
    "    eps= epsilon\n",
    "    \n",
    "    #iteramos en los episodios\n",
    "    for ep in range (episodios):\n",
    "        estado = env.reiniciar()\n",
    "        \n",
    "        #aqui iniciamos la politica epsilon grady (e-grady)\n",
    "        if np.random.rand() < eps:\n",
    "            accion = np.random.randint(n_acciones)\n",
    "        else:\n",
    "            accion = int(np.argmax(Q[estado]))\n",
    "        \n",
    "        siguiente_estado, recompensa, terminado, info = env.paso(accion)\n",
    "        \n",
    "        mejor_siguiente = np.max(Q[siguiente_estado])\n",
    "        td_objetivo = recompensa + gamma * mejor_siguiente\n",
    "        td_error = td_objetivo - Q[estado, accion]\n",
    "        Q[estado, accion] += alpha * td_error\n",
    "\n",
    "        historial_recompensas.append(recompensa)\n",
    "        eps = max(min_epsilon, eps * decay_epsilon)\n",
    "        \n",
    "        if (ep + 1) % 2000 == 0:\n",
    "            prom_reciente = np.mean(historial_recompensas[-5000:])\n",
    "            print(f\"Episodio {ep+1}/{episodios} | recompensa_promedio_ult_{2000}: {prom_reciente:.2f} | eps: {eps:.4f}\")\n",
    "\n",
    "    np.save(guardar_ruta, Q)\n",
    "    print(f\"Entrenamiento finalizado. Q-tabla guardada en: {guardar_ruta}\")\n",
    "    return Q, historial_recompensas\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9796d570",
   "metadata": {},
   "source": [
    "## Entrenamiento del Agente\n",
    "\n",
    "El agente juega miles de veces.  \n",
    "Usa \"epsilon-greedy\": explora acciones nuevas y luego aprovecha lo que ya aprendió.  \n",
    "Con el tiempo comete cada vez menos errores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a8f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 2000/20000 | recompensa_promedio_ult_2000: 6.38 | eps: 0.0670\n",
      "Episodio 4000/20000 | recompensa_promedio_ult_2000: 6.54 | eps: 0.0449\n",
      "Episodio 6000/20000 | recompensa_promedio_ult_2000: 6.65 | eps: 0.0301\n",
      "Episodio 8000/20000 | recompensa_promedio_ult_2000: 6.75 | eps: 0.0202\n",
      "Episodio 10000/20000 | recompensa_promedio_ult_2000: 6.91 | eps: 0.0135\n",
      "Episodio 12000/20000 | recompensa_promedio_ult_2000: 7.01 | eps: 0.0100\n",
      "Episodio 14000/20000 | recompensa_promedio_ult_2000: 7.05 | eps: 0.0100\n",
      "Episodio 16000/20000 | recompensa_promedio_ult_2000: 7.00 | eps: 0.0100\n",
      "Episodio 18000/20000 | recompensa_promedio_ult_2000: 7.00 | eps: 0.0100\n",
      "Episodio 20000/20000 | recompensa_promedio_ult_2000: 7.03 | eps: 0.0100\n",
      "Entrenamiento finalizado. Q-tabla guardada en: q_tabla.npy\n"
     ]
    }
   ],
   "source": [
    "#aumente lo que es el reparable para que ninicie el entrenmeinto con los estaodos\n",
    "Q, recompensas = entrenamiento(episodios=20000, alpha=0.2, gamma=0.99,epsilon=0.1, decay_epsilon=0.9998, min_epsilon=0.01,prob_reparable=0.2,prob_malo=0.3, semilla=123, guardar_ruta='q_tabla.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d91ac",
   "metadata": {},
   "source": [
    "## simulacion con pygame  \n",
    "\n",
    "en esta seccion lo que hacemos es crear un funcion para poder simularlo  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c4b1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.19)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guxs/anaconda3/envs/iapro/lib/python3.10/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "\n",
    "\n",
    "#lo que hice aqui es crear otro tipo de brazo mas para poder hacer que el producto que esta para reparacion vayya a otra parte \n",
    "#como se ve tuve que modificar la mayoria de la silmulacion aumentar prob_bueno=0.5, prob_reparable=0.2, prob_malo=0.3\n",
    "\n",
    "\n",
    "#como tabien agregue los colores para el reparable\n",
    "def simular_pygame(q_tabla_ruta='q_tabla.npy', prob_bueno=0.5, prob_reparable=0.2, prob_malo=0.3):\n",
    "\n",
    "    if not os.path.exists(q_tabla_ruta):\n",
    "        print(f\"Q-tabla no encontrada en {q_tabla_ruta}. Entrena primero.\")\n",
    "        return\n",
    "    \n",
    "    Q = np.load(q_tabla_ruta)\n",
    "\n",
    "    pygame.init()\n",
    "    ANCHO, ALTO = 640, 240\n",
    "    pantalla = pygame.display.set_mode((ANCHO, ALTO))\n",
    "    pygame.display.set_caption('Cinta Transportadora - 3 Estados / 3 Acciones')\n",
    "    reloj = pygame.time.Clock()\n",
    "    FPS = 60\n",
    "\n",
    "   \n",
    "    BLANCO = (255,255,255)\n",
    "    CINTA = (120,120,120)\n",
    "    BUENO = (50,180,50)\n",
    "    REPARABLE = (255, 200, 50)\n",
    "    MALO = (200,60,60)\n",
    "    RETIRADOR = (70,130,180)\n",
    "    REPARADOR = (100, 150, 255)\n",
    "\n",
    "    \n",
    "    x_retirador = ANCHO - 200\n",
    "  \n",
    "    x_reparador = ANCHO - 100\n",
    "\n",
    "   \n",
    "    x_producto = 60\n",
    "    y_producto = ALTO//2\n",
    "    velocidad = 2\n",
    "    rng = random.Random(123)\n",
    "\n",
    "    \n",
    "    def nuevo_estado():\n",
    "        return np.random.choice([0,1,2], p=[prob_bueno, prob_reparable, prob_malo])\n",
    "\n",
    "    estado_producto = nuevo_estado()\n",
    "\n",
    "    corriendo = True\n",
    "    while corriendo:\n",
    "        for evento in pygame.event.get():\n",
    "            if evento.type == pygame.QUIT:\n",
    "                corriendo = False\n",
    "\n",
    "        pantalla.fill(BLANCO)\n",
    "        pygame.draw.rect(pantalla, CINTA, (0, ALTO//2 - 20, ANCHO, 40))\n",
    "\n",
    "        estado = estado_producto\n",
    "        accion = int(np.argmax(Q[estado]))\n",
    "\n",
    "        \n",
    "        if accion == 1:\n",
    "            pygame.draw.rect(pantalla, RETIRADOR, (x_retirador, ALTO//2 - 60, 40, 120))\n",
    "        else:\n",
    "            pygame.draw.rect(pantalla, (180,180,180), (x_retirador, ALTO//2 - 60, 40, 120))\n",
    "\n",
    "      \n",
    "      #si la accion es 2 a reparar\n",
    "        if accion == 2:\n",
    "            pygame.draw.rect(pantalla, REPARADOR, (x_reparador, ALTO//2 - 60, 40, 120))\n",
    "        else:\n",
    "            pygame.draw.rect(pantalla, (180,180,180), (x_reparador, ALTO//2 - 60, 40, 120))\n",
    "\n",
    "      \n",
    "        if estado_producto == 0:\n",
    "            color = BUENO\n",
    "        elif estado_producto == 1:\n",
    "            color = REPARABLE\n",
    "        else:\n",
    "            color = MALO\n",
    "\n",
    "        pygame.draw.circle(pantalla, color, (int(x_producto), y_producto), 16)\n",
    "\n",
    "       \n",
    "        x_producto += velocidad\n",
    "\n",
    "       \n",
    "        if x_producto >= x_retirador + 10:\n",
    "            if accion == 1:   \n",
    "                y_producto -= 4\n",
    "            elif accion == 2 and x_producto >= x_reparador:\n",
    "                y_producto += 4   \n",
    "            else:\n",
    "                y_producto = ALTO // 2\n",
    "\n",
    "       \n",
    "        if x_producto > ANCHO + 50 or y_producto < -50 or y_producto > ALTO + 50:\n",
    "            x_producto = 40\n",
    "            y_producto = ALTO//2\n",
    "            estado_producto = nuevo_estado()\n",
    "\n",
    "        \n",
    "        fuente = pygame.font.SysFont(None, 20)\n",
    "        texto = fuente.render(\n",
    "            'Acciones: 0=dejar   1=retirar   2=reparar', \n",
    "            True, (0,0,0)\n",
    "        )\n",
    "        pantalla.blit(texto, (10,10))\n",
    "\n",
    "        pygame.display.flip()\n",
    "        reloj.tick(FPS)\n",
    "\n",
    "    pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efac265e",
   "metadata": {},
   "source": [
    "## ejecucion de la simulacion  \n",
    "al ejecutar la simulacion lo que estamos haciendo es pasar la tabla Q que tenemos almacenada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "895298cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "simular_pygame('q_tabla.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iapro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
