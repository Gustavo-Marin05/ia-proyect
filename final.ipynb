{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4299869b",
   "metadata": {},
   "source": [
    "# Proyecto: Clasificación de Productos con Aprendizaje por Refuerzo (Q-Learning)\n",
    "\n",
    "Este proyecto simula una cinta transportadora como las que se encuentran en fábricas o plantas industriales.  \n",
    "Un agente inteligente aprende a tomar decisiones por sí mismo sobre:\n",
    "\n",
    "- Cuándo retirar un producto defectuoso.\n",
    "- Cuándo dejar pasar un producto en buen estado.\n",
    "\n",
    "Para esto se utiliza **Aprendizaje por Refuerzo**, específicamente el algoritmo **Q-Learning**, donde el agente aprende mediante recompensas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a378da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356703b7",
   "metadata": {},
   "source": [
    "## Entorno del Proyecto\n",
    "\n",
    "El entorno representa una cinta transportadora con productos:  \n",
    "(estados)\n",
    "- 0 → producto bueno\n",
    "- 1 → producto defectuoso\n",
    "\n",
    "Acciones del agente:\n",
    "- 0 = dejar pasar\n",
    "- 1 = retirar\n",
    "\n",
    "Recompensas:\n",
    "- +10 si toma la decisión correcta\n",
    "- -5 si se equivoca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fcaf55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CintaTransportadoraEnv:\n",
    "\n",
    "    def __init__(self, prob_malo=0.3, semilla=None):\n",
    "        self.prob_malo = prob_malo\n",
    "        self.rng = random.Random(semilla)\n",
    "        self.producto_actual = None\n",
    "\n",
    "    def reiniciar(self):\n",
    "        \n",
    "        self.producto_actual = 1 if self.rng.random() < self.prob_malo else 0\n",
    "        return self.producto_actual\n",
    "\n",
    "    def paso(self, accion):\n",
    "        \n",
    "     \n",
    "   \n",
    "        prod = self.producto_actual\n",
    "        if accion == 1:  # retirar\n",
    "            recompensa = 10.0 if prod == 1 else -5.0\n",
    "        else:            # dejar pasar\n",
    "            recompensa = 5.0 if prod == 0 else -10.0\n",
    "\n",
    "        self.producto_actual = 1 if self.rng.random() < self.prob_malo else 0\n",
    "        terminado = False\n",
    "        info = {\"producto\": prod}\n",
    "        return self.producto_actual, recompensa, terminado, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1540c5",
   "metadata": {},
   "source": [
    "## Algoritmo Q-Learning\n",
    "\n",
    "El agente aprende una tabla Q con valores para cada acción.  \n",
    "Actualiza la tabla con la fórmula:\n",
    "\n",
    "Q(s,a) = Q(s,a) + α [ r + γ max(Q(s’, a’)) − Q(s,a) ]\n",
    "\n",
    "Con esto aprende qué hacer en cada estado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "100118c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenamiento(episodios, alpha, gamma,epsilon, decay_epsilon, min_epsilon,prob_malo, semilla, guardar_ruta):\n",
    "    env = CintaTransportadoraEnv(prob_malo=prob_malo,semilla=semilla)\n",
    "    n_estados=2\n",
    "    n_acciones=2\n",
    "    \n",
    "    #iniciamos en ceros\n",
    "    Q = np.zeros((n_estados,n_acciones),dtype=np.float64)\n",
    "    historial_recompensas=[]\n",
    "    eps= epsilon\n",
    "    \n",
    "    #iteramos en los episodios\n",
    "    for ep in range (episodios):\n",
    "        estado = env.reiniciar()\n",
    "        \n",
    "        #aqui iniciamos la politica epsilon grady\n",
    "        if np.random.rand() < eps:\n",
    "            accion = np.random.randint(n_acciones)\n",
    "        else:\n",
    "            accion = int(np.argmax(Q[estado]))\n",
    "        \n",
    "        siguiente_estado, recompensa, terminado, info = env.paso(accion)\n",
    "        \n",
    "        mejor_siguiente = np.max(Q[siguiente_estado])\n",
    "        td_objetivo = recompensa + gamma * mejor_siguiente\n",
    "        td_error = td_objetivo - Q[estado, accion]\n",
    "        Q[estado, accion] += alpha * td_error\n",
    "\n",
    "        historial_recompensas.append(recompensa)\n",
    "        eps = max(min_epsilon, eps * decay_epsilon)\n",
    "        \n",
    "        if (ep + 1) % 2000 == 0:\n",
    "            prom_reciente = np.mean(historial_recompensas[-5000:])\n",
    "            print(f\"Episodio {ep+1}/{episodios} | recompensa_promedio_ult_{2000}: {prom_reciente:.2f} | eps: {eps:.4f}\")\n",
    "\n",
    "    np.save(guardar_ruta, Q)\n",
    "    print(f\"Entrenamiento finalizado. Q-tabla guardada en: {guardar_ruta}\")\n",
    "    return Q, historial_recompensas\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9796d570",
   "metadata": {},
   "source": [
    "## Entrenamiento del Agente\n",
    "\n",
    "El agente juega miles de veces.  \n",
    "Usa \"epsilon-greedy\": explora acciones nuevas y luego aprovecha lo que ya aprendió.  \n",
    "Con el tiempo comete cada vez menos errores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a8f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 2000/20000 | recompensa_promedio_ult_2000: 4.21 | eps: 0.2681\n",
      "Episodio 4000/20000 | recompensa_promedio_ult_2000: 4.58 | eps: 0.1797\n",
      "Episodio 6000/20000 | recompensa_promedio_ult_2000: 5.19 | eps: 0.1205\n",
      "Episodio 8000/20000 | recompensa_promedio_ult_2000: 5.69 | eps: 0.0807\n",
      "Episodio 10000/20000 | recompensa_promedio_ult_2000: 5.95 | eps: 0.0541\n",
      "Episodio 12000/20000 | recompensa_promedio_ult_2000: 6.12 | eps: 0.0363\n",
      "Episodio 14000/20000 | recompensa_promedio_ult_2000: 6.16 | eps: 0.0243\n",
      "Episodio 16000/20000 | recompensa_promedio_ult_2000: 6.33 | eps: 0.0163\n",
      "Episodio 18000/20000 | recompensa_promedio_ult_2000: 6.38 | eps: 0.0109\n",
      "Episodio 20000/20000 | recompensa_promedio_ult_2000: 6.44 | eps: 0.0100\n",
      "Entrenamiento finalizado. Q-tabla guardada en: q_tabla.npy\n"
     ]
    }
   ],
   "source": [
    "Q, recompensas = entrenamiento(episodios=20000, alpha=0.2, gamma=0.99,epsilon=0.4, decay_epsilon=0.9998, min_epsilon=0.01,prob_malo=0.3, semilla=123, guardar_ruta='q_tabla.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d91ac",
   "metadata": {},
   "source": [
    "## simulacion con pygame  \n",
    "\n",
    "en esta seccion lo que hacemos es crear un funcion para poder simularlo  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "536a829d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.19)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guxs/anaconda3/envs/iapro/lib/python3.10/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "\n",
    "def simular_pygame(q_tabla_ruta='q_tabla.npy', prob_malo=0.3):\n",
    "    if not os.path.exists(q_tabla_ruta):\n",
    "        print(f\"Q-tabla no encontrada en {q_tabla_ruta}. Entrena primero.\")\n",
    "        return\n",
    "    Q = np.load(q_tabla_ruta)\n",
    "\n",
    "    pygame.init()\n",
    "    ANCHO, ALTO = 640, 240\n",
    "    pantalla = pygame.display.set_mode((ANCHO, ALTO))\n",
    "    pygame.display.set_caption('Cinta Transportadora - Simulación Q-Learning')\n",
    "    reloj = pygame.time.Clock()\n",
    "    FPS = 60\n",
    "\n",
    "    BLANCO = (255,255,255)\n",
    "    CINTA = (120,120,120)\n",
    "    BUENO = (50,180,50)\n",
    "    MALO = (200,60,60)\n",
    "    RETIRADOR = (70,130,180)\n",
    "\n",
    "    x_producto = 40\n",
    "    y_producto = ALTO//2\n",
    "    velocidad = 2\n",
    "    rng = random.Random(123)\n",
    "    estado_producto = 1 if rng.random() < prob_malo else 0\n",
    "\n",
    "    corriendo = True\n",
    "    while corriendo:\n",
    "        for evento in pygame.event.get():\n",
    "            if evento.type == pygame.QUIT:\n",
    "                corriendo = False\n",
    "\n",
    "        pantalla.fill(BLANCO)\n",
    "        pygame.draw.rect(pantalla, CINTA, (0, ALTO//2 - 20, ANCHO, 40))\n",
    "        estado = estado_producto\n",
    "        accion = int(np.argmax(Q[estado]))\n",
    "        x_retirador = ANCHO - 200\n",
    "        if accion == 1:\n",
    "            pygame.draw.rect(pantalla, RETIRADOR, (x_retirador, ALTO//2 - 60, 40, 120))\n",
    "        else:\n",
    "            pygame.draw.rect(pantalla, (180,180,180), (x_retirador, ALTO//2 - 60, 40, 120))\n",
    "\n",
    "        color = BUENO if estado_producto == 0 else MALO\n",
    "        pygame.draw.circle(pantalla, color, (int(x_producto), y_producto), 16)\n",
    "        x_producto += velocidad\n",
    "        if x_producto >= x_retirador + 10:\n",
    "            if accion == 1:\n",
    "                y_producto -= 4\n",
    "            else:\n",
    "                y_producto = ALTO//2\n",
    "        if x_producto > ANCHO + 50 or y_producto < -50:\n",
    "            x_producto = 40\n",
    "            y_producto = ALTO//2\n",
    "            estado_producto = 1 if rng.random() < prob_malo else 0\n",
    "\n",
    "        fuente = pygame.font.SysFont(None, 20)\n",
    "        texto = fuente.render('Política Q-table: acción=argmax(Q[estado])  |  acción 0=dejar 1=retirar', True, (0,0,0))\n",
    "        pantalla.blit(texto, (10,10))\n",
    "        pygame.display.flip()\n",
    "        reloj.tick(FPS)\n",
    "\n",
    "    pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efac265e",
   "metadata": {},
   "source": [
    "## ejecucion de la simulacion  \n",
    "al ejecutar la simulacion lo que estamos haciendo es pasar la tabla Q que tenemos almacenada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b258020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simular_pygame('q_tabla.npy')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iapro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
